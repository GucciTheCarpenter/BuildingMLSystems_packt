{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting raw text into a bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = 'data/toy/'\n",
    "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# same beginning as above section:\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'this', u'toy']\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# vectorize our new post as follows:\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "# Note that the count vectors returned by the transform method are sparse. That is,\n",
    "# each vector does not store one count value for each word, as most of those counts\n",
    "# would be zero (post does not contain the word). Instead, it uses the more memory\n",
    "# efficient implementation coo_matrix (for \"COOrdinate\"). Our new post, for instance,\n",
    "# actually contains only two elements:\n",
    "print(new_post_vec)\n",
    "\n",
    "# Via its member toarray(), we can again access full ndarray as follows:\n",
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to use the full array if we want to use it as a vector for similarity\n",
    "# calculations. For the similarity measurement (the naive one), we calculate the\n",
    "# Euclidean distance between the count vectors of the new post and all the old\n",
    "# posts as follows:\n",
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta)\n",
    "\n",
    "# The norm() function calculates the Euclidean norm (shortest distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 -1  1  0 -1  0 -1  1  1  1  1  1  0  1  1  0  1 -1  0 -1  0  1  1  1]\n",
      "\n",
      "4.35889894354\n"
     ]
    }
   ],
   "source": [
    "post1 = X_train.toarray().T[:,0]\n",
    "post2 = X_train.toarray().T[:,1]\n",
    "\n",
    "print post1 - post2\n",
    "print ''\n",
    "print dist_raw(post1, post2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.358898943540674"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# messing around with norm\n",
    "def vec_norm(v):\n",
    "    return sum([x**2 for x in v])**.5\n",
    "                \n",
    "vec_norm(post1-post2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post candidate:\n",
      "\timaging databases\n",
      "\n",
      "### Post 0 distance == 4.000000\n",
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "### Post 1 distance == 1.732051\n",
      "Imaging databases provide storage capabilities.\n",
      "### Post 2 distance == 2.000000\n",
      "Most imaging databases safe images permanently.\n",
      "### Post 3 distance == 1.414214\n",
      "Imaging databases store data.\n",
      "### Post 4 distance == 5.099020\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Post 3 is the best match with a score of 1.414214\n"
     ]
    }
   ],
   "source": [
    "best_post = None\n",
    "best_score = 5000\n",
    "best_i = -1\n",
    "\n",
    "print \"Post candidate:\\n\\t%s\\n\" % (new_post)\n",
    "\n",
    "for i in range(len(posts)):\n",
    "    dist = dist_raw(new_post_vec.toarray(), X_train.toarray().T[:, i])\n",
    "    print \"### Post %i distance == %f\" % (i, dist)\n",
    "    print posts[i]\n",
    "    if dist < best_score:\n",
    "        best_post = \"Post %i\" % (i)\n",
    "        best_score = dist\n",
    "#         best_i = \n",
    "\n",
    "print \"\\n%s is the best match with a score of %f\" % (best_post, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Looking at posts 3 and 4, however, the picture is not so clear any more. Post 4 is the\n",
    "# same as Post 3, duplicated three times. So, it should also be of the same similarity to\n",
    "# the new post as Post 3.\n",
    "# Printing the corresponding feature vectors explains the reason:\n",
    "print(X_train.getrow(3).toarray())\n",
    "print(X_train.getrow(4).toarray())\n",
    "\n",
    "# Obviously, using only the counts of the raw words is too simple. We will have to\n",
    "# normalize them to get vectors of unit length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the word count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
