{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting raw text into a bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = 'data/toy/'\n",
    "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# same beginning as above section:\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'this', u'toy']\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# vectorize our new post as follows:\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "# Note that the count vectors returned by the transform method are sparse. That is,\n",
    "# each vector does not store one count value for each word, as most of those counts\n",
    "# would be zero (post does not contain the word). Instead, it uses the more memory\n",
    "# efficient implementation coo_matrix (for \"COOrdinate\"). Our new post, for instance,\n",
    "# actually contains only two elements:\n",
    "print(new_post_vec)\n",
    "\n",
    "# Via its member toarray(), we can again access full ndarray as follows:\n",
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to use the full array if we want to use it as a vector for similarity\n",
    "# calculations. For the similarity measurement (the naive one), we calculate the\n",
    "# Euclidean distance between the count vectors of the new post and all the old\n",
    "# posts as follows:\n",
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta)\n",
    "\n",
    "# The norm() function calculates the Euclidean norm (shortest distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 -1  1  0 -1  0 -1  1  1  1  1  1  0  1  1  0  1 -1  0 -1  0  1  1  1]\n",
      "\n",
      "4.35889894354\n"
     ]
    }
   ],
   "source": [
    "post1 = X_train.toarray().T[:,0]\n",
    "post2 = X_train.toarray().T[:,1]\n",
    "\n",
    "print post1 - post2\n",
    "print ''\n",
    "print dist_raw(post1, post2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.358898943540674"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# messing around with norm\n",
    "def vec_norm(v):\n",
    "    return sum([x**2 for x in v])**.5\n",
    "                \n",
    "vec_norm(post1-post2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post candidate:\n",
      "\timaging databases\n",
      "\n",
      "### Post 0 distance == 4.000000\n",
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "### Post 1 distance == 1.732051\n",
      "Imaging databases provide storage capabilities.\n",
      "### Post 2 distance == 2.000000\n",
      "Most imaging databases safe images permanently.\n",
      "### Post 3 distance == 1.414214\n",
      "Imaging databases store data.\n",
      "### Post 4 distance == 5.099020\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Post 3 is the best match with a score of 1.414214\n"
     ]
    }
   ],
   "source": [
    "best_post = None\n",
    "best_score = 5000\n",
    "# best_i = -1\n",
    "\n",
    "print \"Post candidate:\\n\\t%s\\n\" % (new_post)\n",
    "\n",
    "for i in range(len(posts)):\n",
    "    dist = dist_raw(new_post_vec.toarray(), X_train.toarray().T[:, i])\n",
    "    print \"### Post %i distance == %f\" % (i, dist)\n",
    "    print posts[i]\n",
    "    if dist < best_score:\n",
    "        best_post = \"Post %i\" % (i)\n",
    "        best_score = dist\n",
    "#         best_i = \n",
    "\n",
    "print \"\\n%s is the best match with a score of %f\" % (best_post, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Looking at posts 3 and 4, however, the picture is not so clear any more. Post 4 is the\n",
    "# same as Post 3, duplicated three times. So, it should also be of the same similarity to\n",
    "# the new post as Post 3.\n",
    "# Printing the corresponding feature vectors explains the reason:\n",
    "print(X_train.getrow(3).toarray())\n",
    "print(X_train.getrow(4).toarray())\n",
    "\n",
    "# Obviously, using only the counts of the raw words is too simple. We will have to\n",
    "# normalize them to get vectors of unit length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create function of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post candidate:\n",
      "\timagine imagining\n",
      "\n",
      "### Post 0 distance == 3.742\n",
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "### Post 1 distance == 2.236\n",
      "Imaging databases provide storage capabilities.\n",
      "### Post 2 distance == 2.449\n",
      "Most imaging databases safe images permanently.\n",
      "\n",
      "Post 1 is the best match with a score of 2.236\n"
     ]
    }
   ],
   "source": [
    "def post_cluster(new_post, prev_posts, dist_measure=dist_raw):\n",
    "    \n",
    "    best_post = None\n",
    "    best_score = 50000\n",
    "    X_train = vectorizer.fit_transform(prev_posts)\n",
    "\n",
    "    print \"Post candidate:\\n\\t%s\\n\" % (new_post)\n",
    "    new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "    for i in range(len(prev_posts)):\n",
    "        dist = dist_measure(new_post_vec.toarray(), X_train.toarray().T[:, i])\n",
    "        print \"### Post %i distance == %.3f\" % (i, dist)\n",
    "        print posts[i]\n",
    "        if dist < best_score:\n",
    "            best_post = \"Post %i\" % (i)\n",
    "            best_score = dist\n",
    "    #         best_i = \n",
    "\n",
    "    print \"\\n%s is the best match with a score of %.3f\" % (best_post, best_score)\n",
    "    \n",
    "post_cluster('imagine imagining', posts[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the word count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "2.0\n",
      "[[ 0.   0.   0.   0.   0.5  0.5  0.   0.5  0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.5  0.   0.   0. ]]\n",
      "\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n",
      "6.0\n",
      "[[ 0.   0.   0.   0.   0.5  0.5  0.   0.5  0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.5  0.   0.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(3).toarray())\n",
    "print sp.linalg.norm(X_train.getrow(3).toarray())\n",
    "print (X_train.getrow(3).toarray())/sp.linalg.norm(X_train.getrow(3).toarray())\n",
    "print ''\n",
    "print(X_train.getrow(4).toarray())\n",
    "print sp.linalg.norm(X_train.getrow(4).toarray())\n",
    "print (X_train.getrow(4).toarray())/sp.linalg.norm(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1)\n",
    "    v2_normalized = v2/sp.linalg.norm(v2)\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post candidate:\n",
      "\timaging databases\n",
      "\n",
      "### Post 0 distance == 1.414\n",
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "### Post 1 distance == 0.857\n",
      "Imaging databases provide storage capabilities.\n",
      "### Post 2 distance == 0.919\n",
      "Most imaging databases safe images permanently.\n",
      "### Post 3 distance == 0.765\n",
      "Imaging databases store data.\n",
      "### Post 4 distance == 0.765\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Post 3 is the best match with a score of 0.765\n"
     ]
    }
   ],
   "source": [
    "# use clustering function\n",
    "post_cluster(new_post, posts, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing less important words\n",
    "\n",
    "Let us have another look at Post 2. Of its words that are not in the new post, we have\n",
    "\"most\", \"safe\", \"images\", and \"permanently\". They are actually quite different in the\n",
    "overall importance to the post. Words such as \"most\" appear very often in all sorts of\n",
    "different contexts, and words such as this are called stop words. They do not carry\n",
    "as much information, and thus should not be weighed as much as words such as\n",
    "\"images\", that don't occur often in different contexts. The best option would be to\n",
    "remove all words that are so frequent that they do not help to distinguish between\n",
    "different texts. These words are called stop words.\n",
    "As this is such a common step in text processing, there is a simple parameter in\n",
    "CountVectorizer to achieve this, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "print sorted(vectorizer.get_stop_words())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'learning', u'machine', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'toy']\n"
     ]
    }
   ],
   "source": [
    "# The new word list is seven words lighter:\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post candidate:\n",
      "\timaging databases\n",
      "\n",
      "### Post 0 distance == 1.414\n",
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "### Post 1 distance == 0.857\n",
      "Imaging databases provide storage capabilities.\n",
      "### Post 2 distance == 0.857\n",
      "Most imaging databases safe images permanently.\n",
      "### Post 3 distance == 0.765\n",
      "Imaging databases store data.\n",
      "### Post 4 distance == 0.765\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Post 3 is the best match with a score of 0.765\n"
     ]
    }
   ],
   "source": [
    "# Without stop words\n",
    "post_cluster(new_post, posts, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "## Installing and using NLTK\n",
    "[already present with Anaconda; nice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "s= nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'graphic', u'imag', u'imag', u'imagin', u'imagin']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.stem(x) for x in [\"graphics\", \"imaging\", \"image\", \"imagination\", \"imagine\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'buy', u'buy', u'bought']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.stem(x) for x in [\"buys\", \"buying\", \"bought\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the vectorizer with NLTK's stemmer\n",
    "add'l source: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HELP: super\n",
    "\n",
    "# Docstring:\n",
    "# super(type, obj) -> bound super object; requires isinstance(obj, type)\n",
    "# super(type) -> unbound super object\n",
    "# super(type, type2) -> bound super object; requires issubclass(type2, type)\n",
    "\n",
    "# Typical use to call a cooperative superclass method:\n",
    "# class C(B):\n",
    "#     def meth(self, arg):\n",
    "#         super(C, self).meth(arg)\n",
    "# Type:      type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'safe', u'storag', u'store', u'stuff', u'toy']\n"
     ]
    }
   ],
   "source": [
    "# we now have one feature less, because \"images\" and \"imaging\" collapsed to one\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post candidate:\n",
      "\timaging databases\n",
      "\n",
      "### Post 0 distance == 1.414\n",
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "### Post 1 distance == 0.857\n",
      "Imaging databases provide storage capabilities.\n",
      "### Post 2 distance == 0.630\n",
      "Most imaging databases safe images permanently.\n",
      "### Post 3 distance == 0.765\n",
      "Imaging databases store data.\n",
      "### Post 4 distance == 0.765\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Post 2 is the best match with a score of 0.630\n"
     ]
    }
   ],
   "source": [
    "post_cluster(new_post, posts, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words on steroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# naive approach to tf-idf\n",
    "import numpy as np\n",
    "\n",
    "def tfidf(word, doc, doc_collection):\n",
    "    # 'tf' NOT normalized by length/sum of terms in 'doc'\n",
    "    tf = len([one for one in doc if word == one])\n",
    "    idf = np.log(len(doc_collection)/float(len([match for match in doc_collection if word in match])))\n",
    "    \n",
    "    ### when I thought I'd be working with strings\n",
    "#     tf = doc.lower().split().count(word.lower())\n",
    "#     idf =  np.log(len(doc_collection)/float(len([match for match in doc_collection if word in match])))\n",
    "\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['a'], ['a', 'b', 'b'], ['a', 'b', 'c']], ['a'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a, abb, abc]\n",
    "D, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.810930216216\n",
      "0.0\n",
      "0.405465108108\n",
      "1.09861228867\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", a, D))\n",
    "\n",
    "print(tfidf(\"b\", abb, D))\n",
    "\n",
    "print(tfidf(\"a\", abc, D))\n",
    "\n",
    "print(tfidf(\"b\", abc, D))\n",
    "\n",
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
